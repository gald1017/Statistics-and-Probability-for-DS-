{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ihu4bNfPLlAV"
   },
   "source": [
    "This notebook consists of two tasks. \n",
    "\n",
    "You should read and run it and  fill all the parts containing the **TODO** words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENU7h1RT4TlG"
   },
   "source": [
    "# Task 1: What influences weight loss [40 points]\n",
    "\n",
    "In this section, you will investigate the factors influencing weight. In particular, we are interested whether amount of smoking now affects body weight after 10 years. The null hypothesis is that amount smoking does not affect it. \n",
    "\n",
    "We will explore the data from US [National Health and Nutrition Examination Survey Data I Epidemiologic Follow-up Study](https://wwwn.cdc.gov/nchs/nhanes/nhefs/default.aspx/). It includes data from persons 25-74 years of age who completed a medical examination in 1971. Follow-up examinations were carried out several years later to investigate the relationships between clinical, nutritional, and behavioral factors.\n",
    "\n",
    "We use a subset of data preprocessed for [Hern√°n MA, Robins JM (2020). Causal Inference: What If. Boca Raton: Chapman & Hall/CRC.](https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/). It includes only people who were smoking in 1971. A codebook describing the meaning of the variables is available as [an Excel table](https://cdn1.sph.harvard.edu/wp-content/uploads/sites/1268/2012/10/NHEFS_Codebook.xls).\n",
    "\n",
    "To test the null hypothesis about effect of smoking intensity on weight loss, you should train a regression model predicting weight in year 1982 (`wt82`) using number of cigarretes smoked per day in 1971 (`smokeintensity`) and control factors from year 1971 that might affect weight later, such as sex (`sex`), age `age`, and, obviously,  weight in 1971 (`wt71`), and test the significance of the coefficients for the variables of interest. \n",
    "\n",
    "You may use any other variables, but only if they are not \"from the future\" (observed later than 1971). \n",
    "\n",
    "If you intent to do feature engineering, such as nonlinear features or interactions, you may want to read [the statsmodels documentation about their formula language](https://www.statsmodels.org/stable/example_formulas.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D_oFO9GqAyIP"
   },
   "outputs": [],
   "source": [
    "!wget https://cdn1.sph.harvard.edu/wp-content/uploads/sites/1268/2017/01/nhefs_excel.zip\n",
    "!unzip nhefs_excel.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "U8FWArlE5jXp"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3f2WPjSu2Q-I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "(1629, 64)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seqn</th>\n",
       "      <th>qsmk</th>\n",
       "      <th>death</th>\n",
       "      <th>yrdth</th>\n",
       "      <th>modth</th>\n",
       "      <th>dadth</th>\n",
       "      <th>sbp</th>\n",
       "      <th>dbp</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>...</th>\n",
       "      <th>birthcontrol</th>\n",
       "      <th>pregnancies</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>hightax82</th>\n",
       "      <th>price71</th>\n",
       "      <th>price82</th>\n",
       "      <th>tax71</th>\n",
       "      <th>tax82</th>\n",
       "      <th>price71_82</th>\n",
       "      <th>tax71_82</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>11368</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>89.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>273.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.167969</td>\n",
       "      <td>1.940186</td>\n",
       "      <td>1.049805</td>\n",
       "      <td>0.549927</td>\n",
       "      <td>0.227875</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>4959</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>139.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.461914</td>\n",
       "      <td>1.850098</td>\n",
       "      <td>1.391113</td>\n",
       "      <td>0.582886</td>\n",
       "      <td>0.612061</td>\n",
       "      <td>0.808228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>266</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>15844</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.346680</td>\n",
       "      <td>1.797363</td>\n",
       "      <td>1.364990</td>\n",
       "      <td>0.571899</td>\n",
       "      <td>0.549316</td>\n",
       "      <td>0.792969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1507</th>\n",
       "      <td>24336</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>119.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>14076</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>117.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>209.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.183594</td>\n",
       "      <td>1.739990</td>\n",
       "      <td>1.102295</td>\n",
       "      <td>0.461975</td>\n",
       "      <td>0.443787</td>\n",
       "      <td>0.640381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>6997</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>83.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>243.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.094727</td>\n",
       "      <td>1.867676</td>\n",
       "      <td>0.944946</td>\n",
       "      <td>0.395996</td>\n",
       "      <td>0.226990</td>\n",
       "      <td>0.548950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>22673</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>145.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.183594</td>\n",
       "      <td>1.739990</td>\n",
       "      <td>1.102295</td>\n",
       "      <td>0.461975</td>\n",
       "      <td>0.443787</td>\n",
       "      <td>0.640381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>20303</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>270.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.569580</td>\n",
       "      <td>1.513428</td>\n",
       "      <td>0.551270</td>\n",
       "      <td>0.230988</td>\n",
       "      <td>0.056198</td>\n",
       "      <td>0.320251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>11880</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>85.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>351.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.099609</td>\n",
       "      <td>1.775391</td>\n",
       "      <td>0.997437</td>\n",
       "      <td>0.417969</td>\n",
       "      <td>0.324585</td>\n",
       "      <td>0.579468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows √ó 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       seqn  qsmk  death  yrdth  modth  dadth    sbp   dbp  sex  age  ...  \\\n",
       "442   11368     0      1   89.0    3.0   18.0  121.0  83.0    0   25  ...   \n",
       "184    4959     0      0    NaN    NaN    NaN  139.0  78.0    1   44  ...   \n",
       "7       266     0      0    NaN    NaN    NaN  100.0  53.0    1   29  ...   \n",
       "640   15844     0      0    NaN    NaN    NaN    NaN   NaN    1   55  ...   \n",
       "1507  24336     0      0    NaN    NaN    NaN  119.0  68.0    1   36  ...   \n",
       "559   14076     0      0    NaN    NaN    NaN  117.0  75.0    0   29  ...   \n",
       "270    6997     1      1   83.0   10.0   29.0    NaN   NaN    0   58  ...   \n",
       "1206  22673     1      0    NaN    NaN    NaN  145.0  91.0    1   54  ...   \n",
       "805   20303     1      0    NaN    NaN    NaN    NaN   NaN    0   55  ...   \n",
       "458   11880     0      1   85.0   12.0   16.0  229.0  94.0    0   50  ...   \n",
       "\n",
       "      birthcontrol  pregnancies  cholesterol  hightax82   price71   price82  \\\n",
       "442              2          NaN        273.0        0.0  2.167969  1.940186   \n",
       "184              0          7.0        275.0        0.0  2.461914  1.850098   \n",
       "7                0          2.0        166.0        NaN       NaN       NaN   \n",
       "640              0          2.0        265.0        0.0  2.346680  1.797363   \n",
       "1507             0          3.0        212.0        NaN       NaN       NaN   \n",
       "559              2          NaN        209.0        0.0  2.183594  1.739990   \n",
       "270              2          NaN        243.0        0.0  2.094727  1.867676   \n",
       "1206             0          2.0        230.0        0.0  2.183594  1.739990   \n",
       "805              2          NaN        270.0        0.0  1.569580  1.513428   \n",
       "458              2          NaN        351.0        0.0  2.099609  1.775391   \n",
       "\n",
       "         tax71     tax82  price71_82  tax71_82  \n",
       "442   1.049805  0.549927    0.227875  0.500000  \n",
       "184   1.391113  0.582886    0.612061  0.808228  \n",
       "7          NaN       NaN         NaN       NaN  \n",
       "640   1.364990  0.571899    0.549316  0.792969  \n",
       "1507       NaN       NaN         NaN       NaN  \n",
       "559   1.102295  0.461975    0.443787  0.640381  \n",
       "270   0.944946  0.395996    0.226990  0.548950  \n",
       "1206  1.102295  0.461975    0.443787  0.640381  \n",
       "805   0.551270  0.230988    0.056198  0.320251  \n",
       "458   0.997437  0.417969    0.324585  0.579468  \n",
       "\n",
       "[10 rows x 64 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel('NHEFS.xls')\n",
    "print(data.shape)\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hLSd38pE3yjC"
   },
   "outputs": [],
   "source": [
    "non_missing = data[data.wt82.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDipdvQPW7Yr"
   },
   "source": [
    "<b>TODO: Visualize and summarize the joint distribution of weight in 1982, weight in 1971, and smoking intensity in 1971. Comment on it. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ItR1YT9zW8g5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gal\\AppData\\Roaming\\Python\\Python37\\site-packages\\numpy\\lib\\histograms.py:824: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  keep = (tmp_a >= first_edge)\n",
      "C:\\Users\\gal\\AppData\\Roaming\\Python\\Python37\\site-packages\\numpy\\lib\\histograms.py:825: RuntimeWarning: invalid value encountered in less_equal\n",
      "  keep &= (tmp_a <= last_edge)\n"
     ]
    }
   ],
   "source": [
    "sns.pairplot(non_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g90VRByiVxCl"
   },
   "source": [
    "<b>TODO: improve the model below by including more and better factors in it</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6dfGfsOA2SgY"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>wt82</td>       <th>  R-squared:         </th> <td>   0.768</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.768</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   2584.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 21 Nov 2020</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>08:43:22</td>     <th>  Log-Likelihood:    </th> <td> -5435.5</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1566</td>      <th>  AIC:               </th> <td>1.088e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1563</td>      <th>  BIC:               </th> <td>1.089e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>           <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>      <td>    7.5582</td> <td>    0.966</td> <td>    7.826</td> <td> 0.000</td> <td>    5.664</td> <td>    9.452</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>wt71</th>           <td>    0.9219</td> <td>    0.013</td> <td>   71.329</td> <td> 0.000</td> <td>    0.897</td> <td>    0.947</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>smokeintensity</th> <td>    0.0298</td> <td>    0.017</td> <td>    1.772</td> <td> 0.077</td> <td>   -0.003</td> <td>    0.063</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>171.805</td> <th>  Durbin-Watson:     </th> <td>   2.025</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 874.035</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.382</td>  <th>  Prob(JB):          </th> <td>1.61e-190</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 6.579</td>  <th>  Cond. No.          </th> <td>    370.</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                   wt82   R-squared:                       0.768\n",
       "Model:                            OLS   Adj. R-squared:                  0.768\n",
       "Method:                 Least Squares   F-statistic:                     2584.\n",
       "Date:                Sat, 21 Nov 2020   Prob (F-statistic):               0.00\n",
       "Time:                        08:43:22   Log-Likelihood:                -5435.5\n",
       "No. Observations:                1566   AIC:                         1.088e+04\n",
       "Df Residuals:                    1563   BIC:                         1.089e+04\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==================================================================================\n",
       "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------\n",
       "Intercept          7.5582      0.966      7.826      0.000       5.664       9.452\n",
       "wt71               0.9219      0.013     71.329      0.000       0.897       0.947\n",
       "smokeintensity     0.0298      0.017      1.772      0.077      -0.003       0.063\n",
       "==============================================================================\n",
       "Omnibus:                      171.805   Durbin-Watson:                   2.025\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              874.035\n",
       "Skew:                           0.382   Prob(JB):                    1.61e-190\n",
       "Kurtosis:                       6.579   Cond. No.                         370.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: add more factors to the model\n",
    "model = smf.ols(data=non_missing, formula='wt82~wt71+smokeintensity').fit()  \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOfI9GWc2xmd"
   },
   "source": [
    "### Interpretation\n",
    "\n",
    "\n",
    "<b>TODO: Write here your conclusions about the impact of the variables on interest on weight loss and the overall analysis your model: \n",
    "*  What is the overall accuracy of the model?\n",
    "*  Which factors do have strong relations to the target variable, and which do not?\n",
    "*  How the values of the significant coefficients may be interpreted? \n",
    "</b>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2nELOqS3vhM"
   },
   "source": [
    "# Task 2: Credit scoring and comparing models [60 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qb73qxhmHj9m"
   },
   "source": [
    "In this part, you should train a logistic regression to predict probability of not returning a loan by a bank client. \n",
    "\n",
    "This problem be focused not on explaining *why* some clients do not return the loan, but only on predicting *whether* they will return it (or, rather, with what probability they don't return). Therefore, we don't *have* to perform statistical tests for the model coefficients. However, these test might still be useful for finding the variables that are really useful for predicting the outcome. \n",
    "\n",
    "To validate the quality of prediction on the new data, we will split the data into the train and test parts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sqsopnb-CFTE"
   },
   "outputs": [],
   "source": [
    "credit_data = pd.read_csv('https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/credit.csv')\n",
    "print(credit_data.shape)\n",
    "y = credit_data['default'] - 1  # get 1 for not returning loan, 0 for returning it\n",
    "print(y.mean())\n",
    "predictors = credit_data.drop('default', axis=1)\n",
    "predictors.sample(5).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yer96KMJC9u"
   },
   "source": [
    "Some of the variables are categorical. To make all the data numerical, one may perform one-hot encoding of the categorical columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yh4MnsrNIiDa"
   },
   "outputs": [],
   "source": [
    "X = pd.get_dummies(predictors, drop_first=False)\n",
    "X.sample(3).T.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uX8mvt87MOPO"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8gv573YOZ5e"
   },
   "source": [
    "Here is the logistic regression model trained on all the variables. However, only for the numerical variables here the test for coefficient significance makes sense. \n",
    "\n",
    "**TODO: interpret the signs and significance of the numerical factors (the first 7, from `months_loan_duration` to `dependents`).**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EO4FZ49tIQMI"
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "full_model = sm.Logit(y_train, X_train).fit()\n",
    "full_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWYem49LU6Fr"
   },
   "source": [
    "Quality of binary classification can be assessed with [ROC AUC score](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc). Here we calculate it for the training and testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXIywyaqTnGG"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print('train ROC AUC: ', roc_auc_score(y_train, full_model.predict(X_train)))\n",
    "print('test  ROC AUC: ', roc_auc_score(y_test, full_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JhUIvlglOLhe"
   },
   "source": [
    "For the categorical variables, their encoded values are linearly dependent, so there is no unique way to estimate the coefficients. We could fix this by dropping one category for each categorical variable, but then we still would need some way to test significance of all the other categories simultaneosly.\n",
    "\n",
    "One way to perform such a test is [likelihood ratio test](https://en.wikipedia.org/wiki/Likelihood-ratio_test). It is based on the [Wilk's theorem](https://en.wikipedia.org/wiki/Wilks%27_theorem): if one model is a restricted version of another model, and both have been trained with maximum likelihood method, and in fact the restricted version is the correct one, then the value $2 (LL_1 - LL_0)$ has asymptotic distribution $\\chi^2_k$, where $LL_1$ and $LL_0$ are log-likelihoods of the full and restricted models, and $k$ is the difference between them in degrees of freedom. \n",
    "\n",
    "In our case, the full model is trained above, and the restricted model below lacks coefficients for the `purpose_` variables. If the null hypothesis is true, and the restricted model is correct (which means that stated purpose of loan does not affect the probability of returning it), then the difference between the log likelihoods of two models will be not too large in comparison with typical walues of $\\chi^2_k$ distribution. Here $k$ equals number of excluded predictiors minus 1, because there has already been one linear dependence between these predictors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "daoy3nASMeT6"
   },
   "outputs": [],
   "source": [
    "excluded_columns = X_train.columns[X_train.columns.str.startswith('purpose_')]\n",
    "print(excluded_columns)\n",
    "change_in_dof = len(excluded_columns) - 1\n",
    "X_train_smaller = X_train.drop(excluded_columns, axis=1)\n",
    "smaller_model = sm.Logit(y_train, X_train_smaller).fit()\n",
    "print(smaller_model.llf) # log likelihood of it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJ7fAO9rM0ww"
   },
   "source": [
    "<b>TODO: Perform the likelihood ratio test for the hypothesis that `smaller_model` is the true version of the `full_model`, i.e. that purpose does not affect probability of returning a loan. \n",
    "\n",
    "Use the `llf` property of both models to get their log likelihood. \n",
    "\n",
    "Calculate the $\\chi^2$ p-value for the test and interpret it.\n",
    "</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UFIJ87rhNJi9"
   },
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "test_statistic = # todo: your code here\n",
    "print(test_statistic)\n",
    "p_value = # todo: your code here\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBcH3r0GNej1"
   },
   "source": [
    "<b>TODO: Apply the same method of likelihood ratio test to the hypothesis that `employment_length` does not affect the probability of returning a loan. \n",
    "</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BMXtR_6ETiP6"
   },
   "outputs": [],
   "source": [
    "# TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4Ls3Ni4bjcF"
   },
   "source": [
    "If one cannot reject the hypothesis that a variable affects the target, it seems reasonable to exclude the variable from the model and hope that the model performance does not fall much. This logic can be used to simplify the model by excluding the not-so-useful factors from it, and it is one of popular methods of feature selection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ed-FOBxmbyVx"
   },
   "source": [
    "<b>Todo: Train a model without all factors for which you accept the hypothesis that they don't affect the outcome:\n",
    "- create a list of factors that you intend to exclude\n",
    "- create new versions of `X_train` and `X_test` without these factors\n",
    "- train the simplified model on `X_train`\n",
    "- evaluate performance of the model with ROC AUC on the train and test sets and compare it with the performance of the whole model. Comment on it. \n",
    " </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dq6_Ss0_d7rh"
   },
   "outputs": [],
   "source": [
    "# TODO: your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3c3T8yUAeEJw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "home_assignment_4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
