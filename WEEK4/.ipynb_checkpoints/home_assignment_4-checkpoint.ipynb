{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ihu4bNfPLlAV"
   },
   "source": [
    "This notebook consists of two tasks. \n",
    "\n",
    "You should read and run it and  fill all the parts containing the **TODO** words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENU7h1RT4TlG"
   },
   "source": [
    "# Task 1: What influences weight loss [40 points]\n",
    "\n",
    "In this section, you will investigate the factors influencing weight. In particular, we are interested whether amount of smoking now affects body weight after 10 years. The null hypothesis is that amount smoking does not affect it. \n",
    "\n",
    "We will explore the data from US [National Health and Nutrition Examination Survey Data I Epidemiologic Follow-up Study](https://wwwn.cdc.gov/nchs/nhanes/nhefs/default.aspx/). It includes data from persons 25-74 years of age who completed a medical examination in 1971. Follow-up examinations were carried out several years later to investigate the relationships between clinical, nutritional, and behavioral factors.\n",
    "\n",
    "We use a subset of data preprocessed for [Hern√°n MA, Robins JM (2020). Causal Inference: What If. Boca Raton: Chapman & Hall/CRC.](https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/). It includes only people who were smoking in 1971. A codebook describing the meaning of the variables is available as [an Excel table](https://cdn1.sph.harvard.edu/wp-content/uploads/sites/1268/2012/10/NHEFS_Codebook.xls).\n",
    "\n",
    "To test the null hypothesis about effect of smoking intensity on weight loss, you should train a regression model predicting weight in year 1982 (`wt82`) using number of cigarretes smoked per day in 1971 (`smokeintensity`) and control factors from year 1971 that might affect weight later, such as sex (`sex`), age `age`, and, obviously,  weight in 1971 (`wt71`), and test the significance of the coefficients for the variables of interest. \n",
    "\n",
    "You may use any other variables, but only if they are not \"from the future\" (observed later than 1971). \n",
    "\n",
    "If you intent to do feature engineering, such as nonlinear features or interactions, you may want to read [the statsmodels documentation about their formula language](https://www.statsmodels.org/stable/example_formulas.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D_oFO9GqAyIP"
   },
   "outputs": [],
   "source": [
    "!wget https://cdn1.sph.harvard.edu/wp-content/uploads/sites/1268/2017/01/nhefs_excel.zip\n",
    "!unzip nhefs_excel.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U8FWArlE5jXp"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3f2WPjSu2Q-I"
   },
   "outputs": [],
   "source": [
    "data = pd.read_excel('NHEFS.xls')\n",
    "print(data.shape)\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLSd38pE3yjC"
   },
   "outputs": [],
   "source": [
    "non_missing = data[data.wt82.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDipdvQPW7Yr"
   },
   "source": [
    "<b>TODO: Visualize and summarize the joint distribution of weight in 1982, weight in 1971, and smoking intensity in 1971. Comment on it. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ItR1YT9zW8g5"
   },
   "outputs": [],
   "source": [
    "# TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g90VRByiVxCl"
   },
   "source": [
    "<b>TODO: improve the model below by including more and better factors in it</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6dfGfsOA2SgY"
   },
   "outputs": [],
   "source": [
    "# TODO: add more factors to the model\n",
    "model = smf.ols(data=non_missing, formula='wt82~wt71+smokeintensity').fit()  \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOfI9GWc2xmd"
   },
   "source": [
    "### Interpretation\n",
    "\n",
    "\n",
    "<b>TODO: Write here your conclusions about the impact of the variables on interest on weight loss and the overall analysis your model: \n",
    "*  What is the overall accuracy of the model?\n",
    "*  Which factors do have strong relations to the target variable, and which do not?\n",
    "*  How the values of the significant coefficients may be interpreted? \n",
    "</b>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2nELOqS3vhM"
   },
   "source": [
    "# Task 2: Credit scoring and comparing models [60 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qb73qxhmHj9m"
   },
   "source": [
    "In this part, you should train a logistic regression to predict probability of not returning a loan by a bank client. \n",
    "\n",
    "This problem be focused not on explaining *why* some clients do not return the loan, but only on predicting *whether* they will return it (or, rather, with what probability they don't return). Therefore, we don't *have* to perform statistical tests for the model coefficients. However, these test might still be useful for finding the variables that are really useful for predicting the outcome. \n",
    "\n",
    "To validate the quality of prediction on the new data, we will split the data into the train and test parts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sqsopnb-CFTE"
   },
   "outputs": [],
   "source": [
    "credit_data = pd.read_csv('https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/credit.csv')\n",
    "print(credit_data.shape)\n",
    "y = credit_data['default'] - 1  # get 1 for not returning loan, 0 for returning it\n",
    "print(y.mean())\n",
    "predictors = credit_data.drop('default', axis=1)\n",
    "predictors.sample(5).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yer96KMJC9u"
   },
   "source": [
    "Some of the variables are categorical. To make all the data numerical, one may perform one-hot encoding of the categorical columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yh4MnsrNIiDa"
   },
   "outputs": [],
   "source": [
    "X = pd.get_dummies(predictors, drop_first=False)\n",
    "X.sample(3).T.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uX8mvt87MOPO"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8gv573YOZ5e"
   },
   "source": [
    "Here is the logistic regression model trained on all the variables. However, only for the numerical variables here the test for coefficient significance makes sense. \n",
    "\n",
    "**TODO: interpret the signs and significance of the numerical factors (the first 7, from `months_loan_duration` to `dependents`).**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EO4FZ49tIQMI"
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "full_model = sm.Logit(y_train, X_train).fit()\n",
    "full_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWYem49LU6Fr"
   },
   "source": [
    "Quality of binary classification can be assessed with [ROC AUC score](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc). Here we calculate it for the training and testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXIywyaqTnGG"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print('train ROC AUC: ', roc_auc_score(y_train, full_model.predict(X_train)))\n",
    "print('test  ROC AUC: ', roc_auc_score(y_test, full_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JhUIvlglOLhe"
   },
   "source": [
    "For the categorical variables, their encoded values are linearly dependent, so there is no unique way to estimate the coefficients. We could fix this by dropping one category for each categorical variable, but then we still would need some way to test significance of all the other categories simultaneosly.\n",
    "\n",
    "One way to perform such a test is [likelihood ratio test](https://en.wikipedia.org/wiki/Likelihood-ratio_test). It is based on the [Wilk's theorem](https://en.wikipedia.org/wiki/Wilks%27_theorem): if one model is a restricted version of another model, and both have been trained with maximum likelihood method, and in fact the restricted version is the correct one, then the value $2 (LL_1 - LL_0)$ has asymptotic distribution $\\chi^2_k$, where $LL_1$ and $LL_0$ are log-likelihoods of the full and restricted models, and $k$ is the difference between them in degrees of freedom. \n",
    "\n",
    "In our case, the full model is trained above, and the restricted model below lacks coefficients for the `purpose_` variables. If the null hypothesis is true, and the restricted model is correct (which means that stated purpose of loan does not affect the probability of returning it), then the difference between the log likelihoods of two models will be not too large in comparison with typical walues of $\\chi^2_k$ distribution. Here $k$ equals number of excluded predictiors minus 1, because there has already been one linear dependence between these predictors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "daoy3nASMeT6"
   },
   "outputs": [],
   "source": [
    "excluded_columns = X_train.columns[X_train.columns.str.startswith('purpose_')]\n",
    "print(excluded_columns)\n",
    "change_in_dof = len(excluded_columns) - 1\n",
    "X_train_smaller = X_train.drop(excluded_columns, axis=1)\n",
    "smaller_model = sm.Logit(y_train, X_train_smaller).fit()\n",
    "print(smaller_model.llf) # log likelihood of it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJ7fAO9rM0ww"
   },
   "source": [
    "<b>TODO: Perform the likelihood ratio test for the hypothesis that `smaller_model` is the true version of the `full_model`, i.e. that purpose does not affect probability of returning a loan. \n",
    "\n",
    "Use the `llf` property of both models to get their log likelihood. \n",
    "\n",
    "Calculate the $\\chi^2$ p-value for the test and interpret it.\n",
    "</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UFIJ87rhNJi9"
   },
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "test_statistic = # todo: your code here\n",
    "print(test_statistic)\n",
    "p_value = # todo: your code here\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBcH3r0GNej1"
   },
   "source": [
    "<b>TODO: Apply the same method of likelihood ratio test to the hypothesis that `employment_length` does not affect the probability of returning a loan. \n",
    "</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BMXtR_6ETiP6"
   },
   "outputs": [],
   "source": [
    "# TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4Ls3Ni4bjcF"
   },
   "source": [
    "If one cannot reject the hypothesis that a variable affects the target, it seems reasonable to exclude the variable from the model and hope that the model performance does not fall much. This logic can be used to simplify the model by excluding the not-so-useful factors from it, and it is one of popular methods of feature selection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ed-FOBxmbyVx"
   },
   "source": [
    "<b>Todo: Train a model without all factors for which you accept the hypothesis that they don't affect the outcome:\n",
    "- create a list of factors that you intend to exclude\n",
    "- create new versions of `X_train` and `X_test` without these factors\n",
    "- train the simplified model on `X_train`\n",
    "- evaluate performance of the model with ROC AUC on the train and test sets and compare it with the performance of the whole model. Comment on it. \n",
    " </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dq6_Ss0_d7rh"
   },
   "outputs": [],
   "source": [
    "# TODO: your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3c3T8yUAeEJw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "home_assignment_4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
